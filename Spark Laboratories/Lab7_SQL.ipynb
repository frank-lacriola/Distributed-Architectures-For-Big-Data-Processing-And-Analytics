{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "temporal-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_path = \"/data/students/bigdata-01QYD/Lab6_DBD/register.csv\"\n",
    "stations_path = \"/data/students/bigdata-01QYD/Lab6_DBD/stations.csv\"\n",
    "outputPath = \"res_out_Lab7_SQL/\"\n",
    "treshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exclusive-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "registerDF = spark.read.load(register_path,\\\n",
    "                            format=\"csv\",\\\n",
    "                            sep=\"\\t\",\\\n",
    "                            header=True,\\\n",
    "                            inferSchema=True)\\\n",
    ".createOrReplaceTempView(\"Register\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hundred-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "registerFiltered = spark.sql(\"SELECT * \\\n",
    "    FROM Register \\\n",
    "    WHERE used_slots <> 0 OR free_slots <> 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "amended-orientation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.fullFunction(free_slots)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fullFunction(free_slots):\n",
    "    if free_slots==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "spark.udf.register(\"full\", fullFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "inappropriate-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "registerFiltered.createOrReplaceTempView(\"RegisterFiltered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "italian-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationTS_DF = spark.sql(\"SELECT station, date_format(timestamp, 'EE') as dayofweek, hour(timestamp) as hour, full(free_slots) as fullstatus \\\n",
    "                          FROM Registerfiltered\").createOrReplaceTempView(\"stationTSWithStatus\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "understanding-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredWithTresholdDF = spark.sql(\"SELECT station, dayofweek, hour, avg(fullstatus) AS criticality \\\n",
    "        FROM stationTSwithstatus \\\n",
    "        GROUP BY station, dayofweek, hour \\\n",
    "        HAVING avg(fullstatus) > \"+str(treshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-hierarchy",
   "metadata": {},
   "source": [
    "##--------------------WE CAN COMPUTE JUST ONE QUERY--------------------\n",
    "selectedPairsDF = spark.sql(\"\"\"SELECT station, date_format(timestamp,'EE') as dayofweek, \n",
    "hour(timestamp) as hour, avg(full(free_slots)) as criticality \n",
    "FROM readings \n",
    "WHERE free_slots<>0 OR used_slots<>0\n",
    "GROUP BY station, date_format(timestamp,'EE'), hour(timestamp)\n",
    "HAVING avg(full(free_slots))>\"\"\"+str(threshold))\n",
    "\n",
    "#---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "exempt-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredWithTresholdDF.createOrReplaceTempView(\"FilteredRegister\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "golden-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationsDF = spark.read.load(stations_path,\\\n",
    "                             format=\"csv\",\\\n",
    "                             sep=\"\\t\",\\\n",
    "                             header=True,\\\n",
    "                             inferSchema=True\n",
    "                            )\\\n",
    ".createOrReplaceTempView(\"Stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cooked-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDF = spark.sql(\"SELECT station, dayofweek, hour, criticality, longitude, latitude \\\n",
    "        FROM filteredregister INNER JOIN Stations ON \\\n",
    "        filteredregister.station = stations.id \\\n",
    "        ORDER BY criticality desc, station, dayofweek, hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "certain-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDF.coalesce(1).write.csv(outputPath, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-basin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
