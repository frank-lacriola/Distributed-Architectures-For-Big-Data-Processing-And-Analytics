{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "grand-romania",
   "metadata": {},
   "source": [
    "# TASK A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "residential-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"/user/s292129/data/exam_ex3_data/PerformanceLog.txt\"\n",
    "outputPath1 = \"res_out_ExExemple3_1/\"\n",
    "outputPath2 = \"res_out_ExExemple3_2/\"\n",
    "CPUthr = 10.0\n",
    "RAMthr = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "electrical-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputRDD = sc.textFile(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "elect-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredRDD = inputRDD.filter(lambda line: line.startswith(\"2018/05/\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "diverse-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpuUsagesCountRDD = filteredRDD.map(lambda line: ((line.split(\",\")[2], line.split(\",\")[1].split(\":\")[0]),\\\n",
    "                                              (line.split(\",\")[3], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "billion-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramUsagesCountRDD = filteredRDD.map(lambda line: ((line.split(\",\")[2], line.split(\",\")[1].split(\":\")[0]),\\\n",
    "                                              (line.split(\",\")[4], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "religious-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSumCount(value_pair1, value_pair2):\n",
    "    currUsage1 = float(value_pair1[0])\n",
    "    currCount1 = value_pair1[1]\n",
    "    \n",
    "    currUsage2 = float(value_pair2[0])\n",
    "    currCount2 = value_pair2[1]\n",
    "    \n",
    "    return (currUsage1 + currUsage2, currCount1 + currCount2)\n",
    "\n",
    "# Alternative: combineByKey( lambda inputElem: (inputElem, 1)\\\n",
    "#                            lambda intermed, inputElem:(intermed[0]+inputElem, intermed[1]+1)\n",
    "#                            lambda intermed1, intermed2: (intermed1[0]+intermed2[0], intermed1[1]+intermed2[1])\n",
    "# after -> mapValues(lambda v: v[0]/v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "naked-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpuSumCountRDD = cpuUsagesCountRDD.reduceByKey(computeSumCount)\n",
    "averageCpuUsageRDD = cpuSumCountRDD.mapValues(lambda v: float(v[0])/v[1])\n",
    "\n",
    "ramSumCountRDD = ramUsagesCountRDD.reduceByKey(computeSumCount)\n",
    "averageRamUsageRDD = ramSumCountRDD.mapValues(lambda v: float(v[0])/v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "angry-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredCpuRDD = averageCpuUsageRDD.filter(lambda pair: pair[1] > CPUthr)\n",
    "filteredRamRDD = averageRamUsageRDD.filter(lambda pair: pair[1] > RAMthr)\n",
    "\n",
    "finalRDD = filteredCpuRDD.join(filteredRamRDD)\n",
    "# the alternative by prof is more efficient (no final join op.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalRDD.keys().saveAsTextFile(outputPath1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-excess",
   "metadata": {},
   "source": [
    "# DATAFRAME BASED SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark.read.load(inputPath,\\\n",
    "                          format=\"csv\",\\\n",
    "                          header=False,\\\n",
    "                          inferSchema=True).withColumnRename(\"c0_\",\"...\")\n",
    "...\n",
    "\n",
    "inputDF.createOrReplaceTempView(\"statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractHour(time):\n",
    "    hour = time.split(\":\")[0]\n",
    "    return hour\n",
    "\n",
    "spark.udf.register(\"extractHour\", extractHour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_vsIDHour = inputDF.sql(\"SELECT vsID, extractHour(time) as Hour \\\n",
    "            FROM statistics \\\n",
    "            WHERE date >= '2018/05/01' AND date <= '2018/05/31' \\\n",
    "            GROUP BY vsID, extractHour(time) \\\n",
    "            HAVING avg(cpuUsage) > \"+CPUthr+\" AND avg(ramUsage) > \"+RAMthr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_vsIDHour.write.csv(outputPath, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-inspection",
   "metadata": {},
   "source": [
    "# TASK B  - RDD Based solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "unlike-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract ((vsID, date, hour), cpuUsage) pairs\n",
    "vsID_dateRDD = filteredRDD.map(lambda line: ((line.split(',')[2], line.split(',')[0], int((line.split(',')[1]).split(':')[0])), \\\n",
    "                               float(line.split(',')[3]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "light-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_maxPerHourRDD = vsID_dateRDD.reduceByKey(lambda v1, v2:\\\n",
    "                                             max(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "british-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_maxPerHourRDD = key_maxPerHourRDD.filter(lambda pair: \\\n",
    "                                    pair[1] > 90 or pair[1] < 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "awful-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Alternative Method #########\n",
    "def mapMaxValue(pair):\n",
    "    vsID = pair[0][0]\n",
    "    date = pair[0][1]\n",
    "    maxValue = pair[1]\n",
    "    if maxValue > 90:\n",
    "        return ((vsID, date), (1, 0))\n",
    "    elif maxValue < 10:\n",
    "        return ((vsID, date), (0, 1))\n",
    "\n",
    "\n",
    "mappedRDD = key_maxPerHourRDD.map(mapMaxValue)\n",
    "keys_countersRDD = mappedRDD.reduceByKey(lambda v1, v2: (v1[0]+v2[0], v1[1]+v2[1]))\n",
    "outputRDD = keys_countersRDD.filter(lambda pair: pair[1][0] >= 8\\\n",
    "                        and pair[1][1] >= 8).keys()\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "related-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_hourMaxRDD = key_maxPerHourRDD.map(lambda pair:\\\n",
    "                                       ((pair[0][0], pair[0][1]),\\\n",
    "                                      (pair[0][2], pair[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "narrative-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedRDD = key_hourMaxRDD.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "hawaiian-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countForUnbalanced(listOfTuples):\n",
    "    hoursGreatUsagesList = []\n",
    "    hoursLowUsagesList = []\n",
    "    for tuple_i in listOfTuples:\n",
    "        \n",
    "        if tuple_i[1] > 90:\n",
    "            if tuple_i[0] not in hoursGreatUsagesList:\n",
    "                hoursGreatUsagesList.append(tuple_i[0])\n",
    "        elif tuple_i[1] < 10:\n",
    "            if tuple_i[0] not in hoursLowUsagesList:\n",
    "                hoursLowUsagesList.append(tuple_i[0])\n",
    "                \n",
    "    return (len(hoursGreatUsagesList), len(hoursLowUsagesList))\n",
    "\n",
    "keys_countersRDD = groupedRDD.mapValues(countForUnbalanced)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "second-yeast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('VS3', '2018/05/01'), (15, 8)), (('VS2', '2018/05/01'), (1, 0))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_countersRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dental-trauma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VS3', '2018/05/01')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputRDD = keys_countersRDD.filter(lambda pair: pair[1][0] >= 8\\\n",
    "                                           and pair[1][1] >= 8).keys()\n",
    "outputRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-evanescence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "diverse-accordance",
   "metadata": {},
   "source": [
    "# Task B, Sparl-SQL solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT vsID, date, extractHour(time) AS Hour,\\\n",
    "           max(cpuUsage) as MaxCPU \\\n",
    "          FROM statistics \\\n",
    "          WHERE date >= '2018/05/01' AND date <= '2018/05/31' \\\n",
    "          GROUP BY vsID, date, Hour \\\n",
    "          HAVING max(cpuUsage) > 90 OR max(cpuUsage) < 10\" )\\\n",
    ".createOrReplaceTempView(\"tempView\")\n",
    "\n",
    "query1 = spark.sql(\"SELECT vsID, date \\\n",
    "           FROM tempView \\\n",
    "           GROUP BY vsID, date \\\n",
    "           WHERE cpuUsage > 90 \\\n",
    "           HAVING count(*) >= 8\")\n",
    "\n",
    "query2 = spark.sql(\"SELECT vsID, date \\\n",
    "           FROM tempView \\\n",
    "           GROUP BY vsID, date \\\n",
    "           WHERE cpuUsage < 10 \\\n",
    "           HAVING count(*) >= 8\")\n",
    "\n",
    "outputDF = query1.intersect(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALTERNATIVE BY PROF\n",
    "def greathan90(value):\n",
    "    if value > 90:\n",
    "        return 1\n",
    "    else \n",
    "        return 0\n",
    "    \n",
    "def lessthan10(value):\n",
    "    if value < 10:\n",
    "        return 1\n",
    "    else \n",
    "        return 0\n",
    "\n",
    "spark.udf.register(...)\n",
    "\n",
    "query = spark.sql(\"...HAVING SUM(greathan90(cpuUsage) >= 8 \\\n",
    "                    AND SUM(lessthan10) >= 8....\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
